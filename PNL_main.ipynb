{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datefinder\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "from textblob import TextBlob\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from pyproj import Proj, transform \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: HTTP Error 403: URLBlocked\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dicc = \"Diccionario.xlsx\"\n",
    "path_obs = \"ObservacionesOTs.xlsx\"\n",
    "redes = \"redes_alcantarillado.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = pd.read_excel(path_dicc,sheet_name='Stopwords')\n",
    "stop_words['Stopwords'] = stop_words['Stopwords'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['&#X0D', '7M', 'A', 'ABAJO', 'ACCIONES', 'ACOSTA', 'ACTA', 'ACTIVIDAD', 'ACTIVIDADES', 'ADALBERTO', 'ADALVERTO', 'ADAN', 'ADICIONAL', 'ADOLFO', 'AGUDELO', 'AL', 'ALBERTO', 'ALCANTARILLADO', 'ALCANTARILLADOS', 'ALVAREZ', 'ALVARO', 'ANDEN', 'ANDÉN', 'ANDREA', 'ANGEL', 'ANS', 'ANTE', 'ANTIGUA', 'ANTONIO', 'APROX', 'APROXIMADAMENTE', 'ARANGO', 'ARBOLEDA', 'ARENAS', 'ARENILLA', 'ARGEMIRO', 'ARIAS', 'ARLEY', 'ARNULFO', 'ARRIBA', 'ARTURO', 'ASI', 'ASÍ', 'ATENCIÓN', 'AUGUSTO', 'AUXILIAR', 'BAJO', 'BASE', 'BASE&#X0D;', 'BEDOYA', 'BENITEZ', 'BERNARDO', 'BODEGAS', 'CABALLERO', 'CADAVID', 'CAMILA', 'CAMILO', 'CAMILO&#X0D', 'CANO', 'CARDONA', 'CARLOS', 'CARMONA', 'CASTRO', 'CELIS', 'CERCA', 'CESAR', 'CÉSAR', 'CHEQUEARLA', 'CLAVE', 'CON', 'CON&#X0D', 'CONCRETO', 'CONDICIONES', 'CONFIGURADO', 'CONTINUA', 'CONTRA', 'CONTRATO', 'CORREA', 'CORRESPONDIENTE', 'COSTADO', 'CUAL', 'CUÁL', 'CUALES', 'CUÁLES', 'CUENTA', 'CUESTA', 'DARIO', 'DAVID', 'DE', 'DEBIDO', 'DEFINA', 'DEFINE', 'DEFINIDO', 'DEFINIÓ', 'DEFINIR', 'DEJA', 'DEJANDO', 'DEJANDO&#X0D;', 'DEJÁNDOSE', 'DEJO', 'DEJÓ', 'DEL', 'DEMÁS', 'DESCARGAR', 'DESCARGÓ', 'DESCARGUE', 'DESDE', 'DESPACHO', 'DEYNER', 'DICHA', 'DICHO', 'DICHO&#X0D;', 'DIEGO', 'DIRECTAMENTE', 'DONDE', 'DÓNDE', 'EDUARDO', 'EDWIN', 'EFECTIVA', 'EFRAIN', 'EIDER', 'EJECUTADAS', 'EJECUTADO', 'EL ', 'ELISEO', 'ELÍSEO', 'EMBARGO', 'EN', 'ENCARGADO', 'ENCONTRABA', 'ENCONTRADA', 'ENCONTRADO', 'ENCONTRAR', 'ENCONTRO', 'ENCONTRÓ', 'ENCUENTRA', 'ENCUENTRO', 'ENRIQUE', 'ENTRE', 'ENTREGA', 'EPM', 'EPM\\\\AECHEVEM', 'EPM\\\\JECHAVEC', 'EPM\\\\MMARINOR', 'EPMY', 'EQUIPO&#X0D;', 'ESCALA', 'ESCALANTE', 'ESPINOSA', 'ESTA', 'ESTADO', 'ESTE', 'EVIDENCIO', 'EVITAR', 'EXCAVA', 'EXCAVACIÓN', 'EXCAVAR', 'EXCAVO', 'EXCAVÓ', 'FECHA', 'FERNANDO', 'FERNEY', 'FIGURA', 'FIN', 'FLOREZ', 'FORMATO', 'FOTOS', 'FRANCISCO', 'FREATICO', 'FREÁTICO', 'FUENTES', 'GALLO', 'GARCES', 'GARCÉS', 'GARCIA', 'GARCÍA', 'GOMEZ', 'GÓMEZ', 'GONZALEZ', 'GONZÁLEZ', 'GPZ', 'GRAN', 'GRANDE', 'GRANDES', 'GRANULAR', 'GRISALES', 'GUAYABAL', 'GUÍA', 'GUILLERMO', 'HABIA', 'HABÍA', 'HACIA', 'HASTA', 'HD', 'HENAO', 'HERNANDEZ', 'HERRERA', 'HILARION', 'HINCAPIE', 'HINCAPIÉ', 'HIZO', 'HOJA', 'HORACIO', 'HOYOS', 'HUNDIMIENTO', 'HURTADO', 'INICIAR', 'INICIO', 'INICIÓ', 'IVAN', 'JAIDER', 'JAIME', 'JAIRO', 'JARAMILLO', 'JAVIER', 'JESUS', 'JIMENEZ', 'JOHANY', 'JOHN', 'JORGE', 'JOSE', 'JUAN', 'JULIO', 'KELLY', 'LA', 'LAS', 'LEANDRO', 'LEON', 'LEÓN', 'LEVANTAMIENTO', 'LIBRE', 'LIGERA', 'LLENO', 'LLENOS', 'LONDOÑO', 'LOPEZ', 'LOS', 'LOS ', 'LUIS', 'MANCO', 'MANTENIMIENTO', 'MARIO', 'MATERIAL', 'MATEUS', 'MAURICIO', 'MEJIA', 'MEJÍA', 'MIGUEL', 'MIRA', 'MIRANDA', 'MISMA', 'MISMAS', 'MODELO', 'MODELOS', 'MOMENTO', 'MONSALVE', 'MONTOYA', 'MONZON', 'MOTIVO', 'MUENTES', 'MUÑOZ', 'MUY', 'NEGRETE', 'NELSON', 'NEVARDO', 'NIVEL', 'NORMALIDAD', 'NORTE', 'NOTABLE', 'NUEVAMENTE', 'OBSERVA', 'OBSERVACION', 'OBSERVACIÓN', 'OBSERVADA', 'OBSERVÓ', 'OCAMPO', 'OCASIONABA', 'OCASIONANDO', 'OCASIONÓ', 'OCCIDENTAL', 'OPERANDO', 'OPERATIVA', 'OPERATIVO', 'OPTIMAS', 'OQUENDO', 'ORIENTAL', 'OSORIO', 'PABLO', 'PALACIO', 'PANESSO', 'PAPELETA', 'PARA', 'PARECE', 'PARECER', 'PARIAS', 'PARRA', 'PARTE', 'PASAR', 'PASARÁ', 'PASÓ', 'PATERNINA', 'PEDRO', 'PENDIENTES', 'PEÑA', 'PEQUEÑA', 'PEQUEÑO', 'PEREZ', 'PERMITIÓ', 'PERMITIR', 'PERO', 'PERSONAL', 'PERTINENTE', 'PERTINENTES', 'PIEDRAHITA', 'PM\\\\AECHEVEM', 'PODER', 'POR', 'PORCENTAJE', 'PORTAL', 'PORTALES', 'PRESENTA', 'PRESENTABA', 'PRESENTO', 'PRESENTÓ', 'PRESTAMO', 'PRÉSTAMO', 'PRINCIPAL', 'PROBLEMA', 'PROBLEMÁTICA', 'PROCEDE', 'PROCEDIÓ', 'PROVOCABA', 'PROVOCANDO', 'PROVOCAR', 'PROVOCÓ', 'PRÓXIMOS', 'PUNTUAL', 'QUE', 'QUEDA', 'QUEDAN', 'QUEDANDO', 'QUEDO', 'QUEDÓ', 'RAMIREZ', 'RAMIRO', 'RAMOS', 'REALIZA', 'REALIZABA', 'REALIZAR', 'REALIZARÁ', 'REALIZARAN', 'REALIZO', 'REALIZÓ', 'REFERENCIA', 'REFERENCIAR', 'REFERENCIARA', 'REFERENCIARÁ', 'REGISTRADA', 'REGISTRO', 'REQUIERE', 'RESPECTIVA', 'RESPECTIVO', 'RESTREPO', 'RICARDO', 'RIOS', 'RODRIGO', 'RODRIGUEZ', 'ROMERO', 'ROMUALDO', 'ROQUE', 'RUEDA', 'SABANETA', 'SALAZAR', 'SAMUEL', 'SANCHEZ', 'SARA', 'SATURADA', 'SATURADO', 'SE', 'SEBASTIAN', 'SEGÚN', 'SELECCIONADO', 'SELECTO', 'SERGIO', 'SERPA', 'SISTEMA', 'SOBRE', 'STEVEN', 'SUAREZ', 'TABARES', 'TALUD', 'TENIA', 'TENÍA', 'TORRES', 'TRAMO', 'TRAS', 'TUIRAN', 'UBICADA', 'UBICADO', 'UN', 'UNA', 'UNAS', 'UNO', 'UNOS', 'USUARIO', 'USUGA', 'VACTOR', 'VASQUEZ', 'VELEZ', 'VÉLEZ', 'VELMER', 'VERIFICA', 'VERIFICO', 'VERIFICÓ', 'VIA', 'VÍA', 'VIO', 'VIÓ', 'VISITA', 'VISITAR', 'VISITO', 'VISITÓ', 'VIVIENDA', 'WILLIAM', 'WILSON', 'Y', 'YA', 'YA&#X0D', 'YANNY', 'YONATHAN', 'ZAPATA', 'ZULETA', 'DEIVI', 'BELLIDO', 'PAVIMENTAR', 'ALGUN', 'ALGÚN', 'ALGUNO', 'ALGUNA', 'ALEJANDRO', 'CONCHA', 'DIEZ', 'CARLOS ', 'SALDARRIAGA', 'ZONA', 'JMONTJAR', 'EDGAR', 'MANUEL', 'BARRIOS', 'AGAMEZ', 'EORLIN', 'PINO', 'INSTANCIA', 'PRIMERO', 'PRIMERA', 'PORTALES', 'PARA', 'EN', 'DA', 'O', 'POR', 'CON']\n"
     ]
    }
   ],
   "source": [
    "stoplist = stop_words['Stopwords'].tolist()\n",
    "vacias = []\n",
    "for i in range(len(stoplist)):\n",
    "  vacias.append(stoplist[i].upper())\n",
    "print(vacias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinonimos = pd.read_excel(path_dicc,sheet_name='Sinonimos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fechas(path):\n",
    "  pattern = \"\\d{4}[/-]\\d{2}[/-]\\d{2}\\s\\d{1,2}:\\d{1,2}:\\d{1,2}\"\n",
    "  dates = re.findall(pattern, path)\n",
    "  fechas = pd.DataFrame(dates,columns=['Fechas'])\n",
    "  if len(fechas) > 1:\n",
    "    fechas = fechas.sort_values('Fechas',ascending=False)\n",
    "    ultima_fecha = fechas.iloc[0].values[0]\n",
    "    penultima_fecha = fechas.iloc[1].values[0]\n",
    "    indice_ultima = fechas.index[fechas['Fechas']==ultima_fecha]\n",
    "    if indice_ultima == 0:\n",
    "      new_line = path[:path.index(ultima_fecha)]\n",
    "    else:\n",
    "      new_line = path[path.index(penultima_fecha):path.index(ultima_fecha)]\n",
    "  else:\n",
    "    new_line = path\n",
    "  return new_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quitar_numericos(path):\n",
    "    patron='[0-9]+'\n",
    "    return re.sub(patron, '', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_caracteres(path):    \n",
    "    patron = '[^A-Za-z0-9]+'\n",
    "    return re.sub(patron, ' ', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mayuscula(path):\n",
    "  obs = path.upper()\n",
    "  return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(path):\n",
    "  words = nltk.word_tokenize(path)\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords(path):\n",
    "  text = [word for word in path if word not in vacias]\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quitar_tildes(path):\n",
    "  a,b = 'áéíóúüñ','aeiouun'\n",
    "  trans = str.maketrans(a,b)\n",
    "  obs = path.translate(trans)\n",
    "  return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_ipid(path):\n",
    "  return path.replace('ipid','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quitar_solicitud(path):\n",
    "    return path.replace('solicitud','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def palabras_vacias(path):\n",
    "  sw = ['SI','RED','EL','LA','AECHEVEM','MMARINOR','JECHAVEC','YRUIZORT','RUBEN','XD','CL','LONDONO','MUNOZ','LEONOR','MARIA','TABORDA','SORLEY', 'MOSQUERA',\n",
    "        'ES','CR','FUE','SU','NINGUNA','FOS','FOTOS','AALVARTA','ARRIETA','SANEAR','SROJASMA','KMORENM','LDELGAD','ARISTIZABAL','DUQUE','FOTOGRAFICO','REVISOR','GERARDO',\n",
    "        'ANDRES','CHALA','WILBER','VEGA','GALEANO','DEBE']\n",
    "  texto = [word for word in path if word not in sw]\n",
    "  return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reemplazar_palabras(path):\n",
    "  for index in range(sinonimos.shape[0]):\n",
    "    palabra = sinonimos.iloc[index, : ][0].lower()\n",
    "    sinonimo = sinonimos.iloc[index, : ][1].lower()\n",
    "    path = path.replace(palabra, sinonimo)\n",
    "  return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizacion(path):\n",
    "  text = ' '.join(path)\n",
    "  tokens = token(text)\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revertir_tokenizacion(path):\n",
    "  text = ' '.join(path)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_ipid(path):\n",
    "    posicion=path.find(\"IPID\")\n",
    "    if posicion != -1:\n",
    "      num_ipid=path[posicion+5:posicion+12]\n",
    "    else:\n",
    "      num_ipid=-1\n",
    "    return (num_ipid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "redes_sigma = pd.read_excel(redes)\n",
    "redes_sigma['IPID'] = redes_sigma['IPID'].astype(str)\n",
    "list_ipid = redes_sigma['IPID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Analitica\\PNL_ordenes\\pnl\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.0.2 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "d:\\Analitica\\PNL_ordenes\\pnl\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 1.0.2 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "d:\\Analitica\\PNL_ordenes\\pnl\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator RandomizedSearchCV from version 1.0.2 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "modelo_efectividad = joblib.load('modelo_entrenado_rforest.pkl')\n",
    "modelo_causa = joblib.load('modelo_entrenado_naivebayes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesta = pd.read_excel(path_obs,sheet_name='Observaciones')\n",
    "gesta_depurado = gesta.loc[:,['[$Dim Degenerada Ordenes].[ID Solicitud]','[$Dim Degenerada Ordenes].[Coordenada X]','[$Dim Degenerada Ordenes].[Coordenada Y]','[$Dim Degenerada Ordenes].[Cuadrilla]',\n",
    "                           '[$Dim Degenerada Ordenes].[Observacion]','[$Efectividad].[Efectividad_1]','[$Efectividad].[Causa No Efectividad]','[$Elemento Infraestructura].[IPID]',\n",
    "                           '[Ordenes].[Costo Total Definitivo]','[Ordenes].[Duracion Ejecución Horas Suma]']]\n",
    "gesta_depurado.rename(columns={'[$Dim Degenerada Ordenes].[ID Solicitud]':'SOLICITUD','[$Dim Degenerada Ordenes].[Coordenada X]':'COORDENADA_X','[$Dim Degenerada Ordenes].[Coordenada Y]':'COORDENADA_Y',\n",
    "                      '[$Dim Degenerada Ordenes].[Cuadrilla]':'CUADRILLA','[$Dim Degenerada Ordenes].[Observacion]':'OBSERVACION','[$Efectividad].[Efectividad_1]':'EFECTIVIDAD',\n",
    "                      '[$Efectividad].[Causa No Efectividad]':'CAUSA_NO_EFECTIVIDAD','[$Elemento Infraestructura].[IPID]':'IPID',\n",
    "                      '[Ordenes].[Costo Total Definitivo]':'COSTO_TOTAL','[Ordenes].[Duracion Ejecución Horas Suma]':'DURACION_HORAS'},inplace=True)\n",
    "gesta_depurado['IPID'] = gesta_depurado['IPID'].astype(str)\n",
    "gesta_depurado['DURACION_HORAS'] = gesta_depurado['DURACION_HORAS'].astype(float) \n",
    "gesta_depurado = gesta_depurado.merge(redes_sigma, left_on=\"IPID\", right_on=\"IPID\", how='left')\n",
    "gesta_depurado['COOR_LAT'].fillna(0,inplace=True)\n",
    "gesta_depurado['COOR_LON'].fillna(0,inplace=True)\n",
    "gesta_depurado = gesta_depurado.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediccion_efectividad(observacion):\n",
    "  REPARA_TUBERIA = []\n",
    "  TUBERIA_MAL = []\n",
    "  PAVIMENTA_REPARA = []\n",
    "  TUBERIA_PAVIMENTA = []\n",
    "  CAMBIA_TUBERIA = []\n",
    "  APIQUE_MARCA = []\n",
    "  TUBERIA_REPARA = []\n",
    "  HUECO_TUBERIA = []\n",
    "  BUENO_FUNCIONA = []\n",
    "  ATIENDE_OT = []\n",
    "  GENERA_OT = []\n",
    "  ACOMETIDA_MAL = []\n",
    "  OT_ATIENDE = []\n",
    "  CARPETA_ASFALTICA = []\n",
    "  ATIENDE_DIRECCION = []\n",
    "  DIRECCION_EVIDENCIA = []\n",
    "  PAVIMENTA_DIRECCION = []\n",
    "  DETERIORADA_CARPETA = []\n",
    "  CAMARA_EMPUJE = []\n",
    "  observacion_fecha = fechas(observacion)\n",
    "  observacion_tildes = quitar_tildes(observacion_fecha)\n",
    "  observacion_sinonimo = reemplazar_palabras(observacion_tildes)\n",
    "  observacion_sinipid = eliminar_ipid(observacion_sinonimo)\n",
    "  observacion_num = quitar_numericos(observacion_sinipid)\n",
    "  observacion_depurada = limpiar_caracteres(observacion_num)\n",
    "  observacion_mayuscula = mayuscula(observacion_depurada)\n",
    "  observacion_token = token(observacion_mayuscula)\n",
    "  observacion_limpia = stopwords(observacion_token)\n",
    "  observacion_vacias = palabras_vacias(observacion_limpia)\n",
    "  observacion_notoken = revertir_tokenizacion(observacion_vacias)\n",
    "  VAL1 = [1 if 'REPARA TUBERIA' in observacion_notoken else 0]\n",
    "  REPARA_TUBERIA.extend(VAL1)\n",
    "  VAL2 = [1 if 'TUBERIA MAL' in observacion_notoken else 0]\n",
    "  TUBERIA_MAL.extend(VAL2)\n",
    "  VAL3 = [1 if 'PAVIMENTA REPARA' in observacion_notoken else 0]\n",
    "  PAVIMENTA_REPARA.extend(VAL3)\n",
    "  VAL4 = [1 if 'TUBERIA PAVIMENTA' in observacion_notoken else 0]\n",
    "  TUBERIA_PAVIMENTA.extend(VAL4)\n",
    "  VAL5 = [1 if 'CAMBIA TUBERIA' in observacion_notoken else 0]\n",
    "  CAMBIA_TUBERIA.extend(VAL5)\n",
    "  VAL6 = [1 if 'APIQUE MARCA' in observacion_notoken else 0]\n",
    "  APIQUE_MARCA.extend(VAL6)\n",
    "  VAL8 = [1 if 'TUBERIA REPARA' in observacion_notoken else 0]\n",
    "  TUBERIA_REPARA.extend(VAL8)\n",
    "  VAL9 = [1 if 'HUECO TUBERIA' in observacion_notoken else 0]\n",
    "  HUECO_TUBERIA.extend(VAL9)\n",
    "  VAL10 = [1 if 'BUENO_FUNCIONA' in observacion_notoken else 0]\n",
    "  BUENO_FUNCIONA.extend(VAL10)\n",
    "  VAL11 = [1 if 'ATIENDE OT' in observacion_notoken else 0]\n",
    "  ATIENDE_OT.extend(VAL11)\n",
    "  VAL12 = [1 if 'GENERA OT' in observacion_notoken else 0]\n",
    "  GENERA_OT.extend(VAL12)\n",
    "  VAL13 = [1 if 'ACOMETIDA MAL' in observacion_notoken else 0]\n",
    "  ACOMETIDA_MAL.extend(VAL13)\n",
    "  VAL14 = [1 if 'OT ATIENDE' in observacion_notoken else 0]\n",
    "  OT_ATIENDE.extend(VAL14)\n",
    "  VAL15 = [1 if 'CARPETA ASFALTICA' in observacion_notoken else 0]\n",
    "  CARPETA_ASFALTICA.extend(VAL15)\n",
    "  VAL16 = [1 if 'ATIENDE DIRECCION' in observacion_notoken else 0]\n",
    "  ATIENDE_DIRECCION.extend(VAL16)\n",
    "  VAL17 = [1 if 'DIRECCION EVIDENCIA' in observacion_notoken else 0]\n",
    "  DIRECCION_EVIDENCIA.extend(VAL17)\n",
    "  VAL18 = [1 if 'PAVIMENTA DIRECCION' in observacion_notoken else 0]\n",
    "  PAVIMENTA_DIRECCION.extend(VAL18)\n",
    "  VAL19 = [1 if 'DETERIORADA CARPETA' in observacion_notoken else 0]\n",
    "  DETERIORADA_CARPETA.extend(VAL19)\n",
    "  VAL20 = [1 if 'CAMARA EMPUJE' in observacion_notoken else 0]\n",
    "  CAMARA_EMPUJE.extend(VAL20)\n",
    "\n",
    "  dataframe_test=pd.DataFrame({\"REPARA_TUBERIA\":REPARA_TUBERIA,\n",
    "                \"TUBERIA_MAL\":TUBERIA_MAL,\n",
    "                \"PAVIMENTA_REPARA\":PAVIMENTA_REPARA,\n",
    "                \"TUBERIA_PAVIMENTA\":TUBERIA_PAVIMENTA,\n",
    "                \"CAMBIA_TUBERIA\":CAMBIA_TUBERIA,\n",
    "                \"HUECO_TUBERIA\":HUECO_TUBERIA,\n",
    "                \"APIQUE_MARCA\":APIQUE_MARCA,\n",
    "                \"TUBERIA_REPARA\":TUBERIA_REPARA,\n",
    "                \"HUECO_TUBERIA\":HUECO_TUBERIA,\n",
    "                \"BUENO_FUNCIONA\":BUENO_FUNCIONA,\n",
    "                \"ATIENDE_OT\":ATIENDE_OT,\n",
    "                \"GENERA_OT\":GENERA_OT,\n",
    "                \"ACOMETIDA_MAL\":ACOMETIDA_MAL,\n",
    "                \"OT_ATIENDE\":OT_ATIENDE,\n",
    "                \"CARPETA_ASFALTICA\":CARPETA_ASFALTICA,\n",
    "                \"ATIENDE_DIRECCION\":ATIENDE_DIRECCION,\n",
    "                \"DIRECCION_EVIDENCIA\":DIRECCION_EVIDENCIA,\n",
    "                \"PAVIMENTA_DIRECCION\":PAVIMENTA_DIRECCION,\n",
    "                \"DETERIORADA_CARPETA\":DETERIORADA_CARPETA,\n",
    "                \"CAMARA_EMPUJE\":CAMARA_EMPUJE,})\n",
    "  efectividad_pred = modelo_efectividad.predict(dataframe_test)[0]\n",
    "  if efectividad_pred == 0:\n",
    "    valor = \"NO\"\n",
    "  else:\n",
    "    valor = \"SI\"\n",
    "  return(valor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparacion_ipid(observacion, ipid_desc, efec):\n",
    "  ipid_obs = extraer_ipid(observacion)\n",
    "  if ipid_obs != -1:\n",
    "    if efec != 'NO':\n",
    "      if ipid_obs != ipid_desc:\n",
    "        resultado = \"Diferente\"\n",
    "      else:\n",
    "        resultado = \"Mismo ipid\"\n",
    "    else:\n",
    "      resultado = \"No aplica\"\n",
    "  else:\n",
    "    resultado = \"No aplica\"\n",
    "  return(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparacion_listaipid(efectividad,ipid):\n",
    "  if efectividad == \"SI\":\n",
    "    if ipid in list_ipid:\n",
    "      resultado = \"está\"\n",
    "    else:\n",
    "      resultado = \"no está\"\n",
    "  else:\n",
    "    resultado = \"no aplica\"\n",
    "  return(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculo_distancia(coordenada_x, coordenada_y,coor_lat,coor_long):\n",
    "  inProj = Proj(init='epsg:4236') \n",
    "  outProj = Proj(init='epsg:3116')\n",
    "  if coor_lat > 0:\n",
    "    x1,y1= transform(inProj,outProj,coordenada_x,coordenada_y)\n",
    "    x2,y2= transform(inProj,outProj,coor_long,coor_lat)\n",
    "    xdis = abs(x1 - x2)\n",
    "    ydis = abs(y1 - y2)\n",
    "    distancia = np.sqrt((xdis**2)+(ydis**2))\n",
    "  else:\n",
    "    distancia = 0\n",
    "  return(distancia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diferencia_costo(efectividad, costo):\n",
    "  if efectividad == \"SI\":\n",
    "    if costo < 100000:\n",
    "      inconsistencia = \"SI\"\n",
    "    else:\n",
    "      inconsistencia = \"NO\"\n",
    "  else:\n",
    "      inconsistencia = \"NO\"\n",
    "  return(inconsistencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_tiempo(efectividad, tiempo):\n",
    "    if efectividad == \"SI\":\n",
    "      if tiempo < 0.5:\n",
    "        inconsistencia = \"SI\"\n",
    "      else:\n",
    "        inconsistencia = \"NO\"\n",
    "    else:\n",
    "        inconsistencia = \"NO\"\n",
    "    return(inconsistencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causa_noefec(observacion_ultima):\n",
    "  resultado = modelo_causa.classify(observacion_ultima)\n",
    "  return(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(path):\n",
    "  indicador_lista = []\n",
    "  efectividad_prediccion = []\n",
    "  comparacion_ipid_obs = []\n",
    "  causa_prediccion = []\n",
    "  ipid_modelored = []\n",
    "  lista_distancias = []\n",
    "  resultado_costo = []\n",
    "  resultado_tiempo = []\n",
    "  for index in range(path.shape[0]):\n",
    "    indicador = 0\n",
    "    coordenada_y = path.iloc[index, : ][2]\n",
    "    coordenada_x = path.iloc[index, : ][1]\n",
    "    observacion = path.iloc[index, : ][4]\n",
    "    observacion_ultima = fechas(observacion)\n",
    "    efectividad = path.iloc[index, : ][5]\n",
    "    causa_noefectividad = path.iloc[index, : ][6]\n",
    "    ipid_descargado = path.iloc[index, : ][7]\n",
    "    costo = path.iloc[index, : ][8]\n",
    "    duracion = path.iloc[index, : ][9]\n",
    "    coor_lat = path.iloc[index, : ][10]\n",
    "    coor_long = path.iloc[index, : ][11]\n",
    "    ##Prediccion efectividad\n",
    "    efectividad_pred = prediccion_efectividad(observacion)\n",
    "    efectividad_prediccion.append(efectividad_pred)\n",
    "    if efectividad_pred != efectividad:\n",
    "      indicador += 1\n",
    "    else:\n",
    "      pass\n",
    "    ##Prediccion causa no efectividad\n",
    "    if efectividad == \"NO\":\n",
    "      causa_pred = modelo_causa.classify(observacion_ultima)\n",
    "      causa_prediccion.append(causa_pred)\n",
    "      if causa_pred != causa_noefectividad:\n",
    "        indicador += 1\n",
    "      else:\n",
    "        pass\n",
    "    else:\n",
    "      causa_prediccion.append(\"NA\")\n",
    "    ##Comparar ipid\n",
    "    resultado_compipid = comparacion_ipid(observacion,ipid_descargado,efectividad)\n",
    "    comparacion_ipid_obs.append(resultado_compipid)\n",
    "    if resultado_compipid == \"Diferente\":\n",
    "      indicador += 1\n",
    "    else:\n",
    "      pass\n",
    "    ##Comparar ipid modelo de red\n",
    "    resultado_modelored = comparacion_listaipid(efectividad, ipid_descargado)\n",
    "    ipid_modelored.append(resultado_modelored)\n",
    "    if resultado_modelored == \"no está\":\n",
    "      indicador += 2\n",
    "    else:\n",
    "      pass\n",
    "    #Diferencia costo\n",
    "    resultado_dif_costo = diferencia_costo(efectividad, costo)\n",
    "    resultado_costo.append(resultado_dif_costo)\n",
    "    if resultado_dif_costo == \"SI\":\n",
    "      indicador += 1\n",
    "    else:\n",
    "      pass\n",
    "    #Control tiempo\n",
    "    diferencia_tiempo = control_tiempo(efectividad, duracion)\n",
    "    resultado_tiempo.append(diferencia_tiempo)\n",
    "    if diferencia_tiempo == \"SI\":\n",
    "      indicador += 1\n",
    "    else:\n",
    "      pass\n",
    "    #Control distancias\n",
    "    resultado_distancia = calculo_distancia(coordenada_x, coordenada_y,coor_lat,coor_long)\n",
    "    lista_distancias.append(resultado_distancia)\n",
    "    if resultado_distancia >= 100:\n",
    "      indicador += 1\n",
    "    else:\n",
    "      pass\n",
    "    indicador_lista.append(indicador)\n",
    "  path['Calidad_ot'] = [\"Buena\" if indicador>0 else \"Mala\" for indicador in path['indicador']]\n",
    "  gesta_depurado['Value'] = 1\n",
    "  path['efectividad_predecida'] = efectividad_prediccion\n",
    "  path['causa_noefectividad_predecida'] = causa_prediccion\n",
    "  path['comparacion_ipid_obs'] = comparacion_ipid_obs\n",
    "  path['ipid_modelored'] = ipid_modelored\n",
    "  path['distancias_ipid'] = lista_distancias\n",
    "  path['inconsistencia_costo'] = resultado_costo\n",
    "  path['inconsistencia_tiempo'] = resultado_tiempo\n",
    "  path['indicador'] = indicador_lista\n",
    "  return(path.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\squintra/nltk_data'\n    - 'd:\\\\Analitica\\\\PNL_ordenes\\\\pnl\\\\nltk_data'\n    - 'd:\\\\Analitica\\\\PNL_ordenes\\\\pnl\\\\share\\\\nltk_data'\n    - 'd:\\\\Analitica\\\\PNL_ordenes\\\\pnl\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\squintra\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Celda 32\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X43sdW50aXRsZWQ%3D?line=0'>1</a>\u001b[0m main(gesta_depurado)\n",
      "\u001b[1;32mUntitled-1.ipynb Celda 32\u001b[0m in \u001b[0;36mmain\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X43sdW50aXRsZWQ%3D?line=21'>22</a>\u001b[0m coor_long \u001b[39m=\u001b[39m path\u001b[39m.\u001b[39miloc[index, : ][\u001b[39m11\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X43sdW50aXRsZWQ%3D?line=22'>23</a>\u001b[0m \u001b[39m##Prediccion efectividad\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X43sdW50aXRsZWQ%3D?line=23'>24</a>\u001b[0m efectividad_pred \u001b[39m=\u001b[39m prediccion_efectividad(observacion)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X43sdW50aXRsZWQ%3D?line=24'>25</a>\u001b[0m efectividad_prediccion\u001b[39m.\u001b[39mappend(efectividad_pred)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X43sdW50aXRsZWQ%3D?line=25'>26</a>\u001b[0m \u001b[39mif\u001b[39;00m efectividad_pred \u001b[39m!=\u001b[39m efectividad:\n",
      "\u001b[1;32mUntitled-1.ipynb Celda 32\u001b[0m in \u001b[0;36mprediccion_efectividad\u001b[1;34m(observacion)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X43sdW50aXRsZWQ%3D?line=25'>26</a>\u001b[0m observacion_depurada \u001b[39m=\u001b[39m limpiar_caracteres(observacion_num)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X43sdW50aXRsZWQ%3D?line=26'>27</a>\u001b[0m observacion_mayuscula \u001b[39m=\u001b[39m mayuscula(observacion_depurada)\n\u001b[1;32m---> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X43sdW50aXRsZWQ%3D?line=27'>28</a>\u001b[0m observacion_token \u001b[39m=\u001b[39m token(observacion_mayuscula)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X43sdW50aXRsZWQ%3D?line=28'>29</a>\u001b[0m observacion_limpia \u001b[39m=\u001b[39m stopwords(observacion_token)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X43sdW50aXRsZWQ%3D?line=29'>30</a>\u001b[0m observacion_vacias \u001b[39m=\u001b[39m palabras_vacias(observacion_limpia)\n",
      "\u001b[1;32mUntitled-1.ipynb Celda 32\u001b[0m in \u001b[0;36mtoken\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X43sdW50aXRsZWQ%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtoken\u001b[39m(path):\n\u001b[1;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X43sdW50aXRsZWQ%3D?line=1'>2</a>\u001b[0m   words \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mword_tokenize(path)\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X43sdW50aXRsZWQ%3D?line=2'>3</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m words\n",
      "File \u001b[1;32md:\\Analitica\\PNL_ordenes\\pnl\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32md:\\Analitica\\PNL_ordenes\\pnl\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32md:\\Analitica\\PNL_ordenes\\pnl\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32md:\\Analitica\\PNL_ordenes\\pnl\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[1;32md:\\Analitica\\PNL_ordenes\\pnl\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\squintra/nltk_data'\n    - 'd:\\\\Analitica\\\\PNL_ordenes\\\\pnl\\\\nltk_data'\n    - 'd:\\\\Analitica\\\\PNL_ordenes\\\\pnl\\\\share\\\\nltk_data'\n    - 'd:\\\\Analitica\\\\PNL_ordenes\\\\pnl\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\squintra\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "main(gesta_depurado)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 ('pnl': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef2dff11370c1bc413ac684edc8fdec0f3baf1b915efc9258e68a3d60bdee522"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
